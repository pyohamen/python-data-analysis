## 1.2 멕시코풍 프랜차이즈 chipotle 의 주문 데이터 분석하기

> 주문데이터를 불러와 기초정보 살펴보고 탐색 / 시각화를 수행해보고 이 예제로 전처리를 해보고 이런저런 인사이트를 살펴보자
>
> 흐름은 순차적으로 내가 원하고자 하는 넓은 범위 조건의 df 를 만들고, 계속해서 df 를 만들어가면서 결국 원하는 specific 한 조건의 df 를 만드는 듯

### Step 1. 탐색: 데이터의 기초 정보 살펴보기

- 불러오기

  - Csv / tsp 등 파일을 df 객체로 바꿔주자

- 정보확인

  - 전체적인 정보, 통계적 정보 등등을 파악할 수 있다.

- 수치적 특성들 파악

  - 수치의 요약적인 통계량 파악가능
  - 피처의 value 수를 중복제거하여 파악할 수 있다.

  

### Step2. 인사이트의 발견: 탐색과 시각화

- 가장 많이 주문한 item top 10
- Value_count() 사용
- Item 당 주문 개수
- groupby() 사용
- 아이템당 주문개수 시각화

  - matplot 라이브러리 사용

- ! Value_counts() 와 unique() 의 차이
  - Value_count() 는 series 를 반환
  - Unique() 는 numpy 배열을 반환



### Step3. 데이터 전처리

- Item_price 피처가 $--- 인 value 이기 때문에, apply 함수를 이용해 $를 없앤 int64 로 바꿔줄 수 있다.




### Step4. 이런저런 인사이트

- 주문당 평균 계산금액

  - groupby('order_id') 중 [item_price] 의 sum()의 mean()

- 한 주문에 10달러 이상 사용한 주문 번호 출력

  - Groupby('order_id') 의 sum() 중 [그 중 item_price 가 10 이상인 것]

- 각 아이템의 가격 구하기

  - Groupby('item_name') 의 min() 중 ['item_price']

- 아이템 가격 분포 그래프 출력

  - plt 를 이용해서 x_pos 를 각 아이템으로 주고 y_pos 를 item_prce 를 holist() 해줘서 넣어주자

- 가장 비싼 주문에서 아이템의 개수가 몇개인지

  - Groupby('order_id') 의 sum() 을 sort_values(by='item_price', ascending=False) 해주자

- "Veggie Salad Bowl" 이 몇 번 주문되었는지 확인

  - 조건문에서 'item_name' 이 'veggie salad bowl' 인 것만 따로 df 를 만들어주고 drop_duplicates('item_name', 'order_id') 해서 중복제거

- Chicken Bowl 을 2개 이상 주문한 주문 횟수 수하기. 

  - Quantity 가 2 이상인 것

- Chicken Bowl 을 2 개 이상 주문한 고객들의 "Chicken Bowl" 메뉴의 총 주문 수량

  - 총합을 나타내는 df 를 만든 후 그 중 값이 2 이상인 것만 조건문

  

## 1.3 국가별 음주 데이터 분석하기

> 위 chipo 와 비슷하니, 이제 좀 더 라이브러리에 대해 익숙해져보자

### Step 1 탐색: 데이터의 기초 정보 살펴보기

- 데이터셋 기본 정보 가져오기

- 정보들을 한 번 보자

  

### Step 2 인사이트 발견: 탐색과 시각화하기

- 피처 간 상관관계

  > #### 상관분석
  >
  > 상관 분석이란 두 변수 간의 선형적 관계를 상관 계수로 표현하는 것
  > 상관 계수를 구하는 것은 공분산의 개념을 포함
  > 공분산: 2개의 확률 변수에 대한 상관 정도, 2개의 변수 중 하나의 값이 상승할 때 다른 값이 얼마나 상승하는지의 수치
  > But, 공분산만으로 구하면 두 변수의 단위 크기에 영향을 받을 수 있어서 -1 과 1 사이인 상관계수로 바꾼다.
  > 1에 가깝다면 양의 상관관계, -1에 가깝다면 음의 상관관계

  - df.corr(method = 'pearson') 을 주로 쓴다.

- corr 행렬 히트맵 시각화하기

  > [Seaboard.heatmap 의 parameters](https://seaborn.pydata.org/generated/seaborn.heatmap.html)

- 피처 간 scatter plot 출력

  > [seaborn.pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)



### Step3. 탐색적 분석: 스무고개로 분석

- continent 피처에 대한 결측 (누락) 데이터 전처리

  - na 되어 있는 value 들을 처리해줄 수 있다. Fillna('원하는값')

- 파이차트로 시각화

  > [plt.pie](https://matplotlib.org/3.3.4/api/_as_gen/matplotlib.pyplot.pie.html)
  >
  > **labels**
  >
  > list, default: None
  > A sequence of strings providing the labels for each wedge
  >
  > **x**
  >
  > 수치로 나타낼 값

- 대륙별 분석

  - Agg() 로 원하는 method 를 한데 모을 수 있다.
  - 전체 평균값을 따로 구해서 전체 평균보다 높은 것을 조건문 할 수 있다.
  - Idxmax() 를 이용해서 value 가 가장 높은 index 를 구할 수 있다.

- 분석결과 시각화

  - plt.xticks 으로 tick 을 몇 개나, 또 어떤 label 로 표현할지 조절할 수 있다.
  - plt.legend() : 범례
  - plt.plot() 을 이용해 그래프 위에 표시할 수 있다.
  - plt.set_color(): bar 의 색 설정 가능



### Step4. 통계적 분석: 분석 대상간의 통계적 차이 검정하기

> 지금까지 두 피처 간의 상관성이나, 그룹 단위로 나누어 수치 정보를 살펴봤다. 물론 유용하지만,
>
> 이는 분석가의 주관에 따라 도출된 내용이여서 **분석의 타당성** 에서 한계가 있다.
>
> 따라서 통계적으로 차이를 검정하는 과정이 필요하고, 그 중 가장 기본적인 방법인 t-test 를 해보자.

[통계학노트](/통계학노트.md)



#### 대한민국은 얼마나 술을 독하게 마시는 나라일까?

- 술 소비량 대비 알코올 비율 피처를 생성하면 알 수 있다.



#### 표로 정리

| 데이터 탐색 질문                               | 핵심 내용                    | 인사이트                                                     |
| ---------------------------------------------- | ---------------------------- | ------------------------------------------------------------ |
| 각각의 피처는 서로 어떤 상관 관계를 갖는가?    | 모든 연속형 피처의 상관 분석 | 대부분의 국가의 총 알코올 소비량은 맥주 소비량에 영향을 받을 확률이 높다. 또한 대부분의 국가에서는 맥주가 가장 많이 소비되는 술이라는 해석도 가능 |
| 평균 맥주 소비량이 가장 높은 대륙?             | 모든 행을 그룹 단위로 분석   | 유럽이 가장 맥주 소비량이 높음. 대륙별로 상이한 차이가 있다는 것을 발견함 |
| 술 소비량 대비 알코올 비율 피처 생성하기       | 새로운 분석 피처 생성        | 술 소비량 대비 알코올 비율이라는 새로운 피처로부터 술을 독하게 마시는 정도의 국가별 차이를 관찰 가능 |
| 아프리카와 유럽 간의 맥주 소비량 차이 **검정** | 통계적 차이 검정             | T-test 분석 결과, 아프리카와 유럽 간의 맥주 소비량은 통계적으로 유의미한 차이를 보임 (단, 그룹 간 데이터 크기가 매우 다르고, 정규분포를 띤다는 가정을 할 수 없어 신뢰할 만한 정보라고 할 수 없음.) |

- 